> 软件开发都是取舍的问题 这就是软件开发，按下了葫芦起了瓢。
# [酷壳儿分布式事务处理](https://coolshell.cn/articles/10910.html)
  * 单点故障
    1. 数据分区
      如果A帐号和B帐号的数据不在同一台服务器上怎么办？我们需要一个跨机器的事务处理。也就是说，如果A的扣钱成功了，但B的加钱不成功，我们还要把A的操作给回滚回去。这在跨机器的情况下，就变得比较复杂了
    2. 数据镜像
      A帐号和B帐号间的汇款是可以在一台机器上完成的，但是别忘了我们有多台机器存在A帐号和B帐号的副本。如果对A帐号的汇钱有两个并发操作（要汇给B和C），这两个操作发生在不同的两台服务器上怎么办？也就是说，在数据镜像中，在不同的服务器上对同一个数据的写操作怎么保证其一致性，保证数据不冲突？
    同时，我们还要考虑性能的因素，如果不考虑性能的话，事务得到保证并不困难，系统慢一点就行了。除了考虑性能外，我们还要考虑可用性，也就是说，一台机器没了，数据不丢失，服务可由别的机器继续提供。 于是，我们需要重点考虑下面的这么几个情况
    1. 容灾：数据不丢、结点的Failover
    2. 数据的一致性：事务处理
    3. 性能：吞吐量 、 响应时间

  * 一致性模型
  说起数据一致性来说，简单说有三种类型（当然，如果细分的话，还有很多一致性模型，如：顺序一致性，FIFO一致性，会话一致性，单读一致性，单写一致性，但为了本文的简单易读，我只说下面三种）：
  1. Weak 弱一致性：当你写入一个新值后，读操作在数据副本上可能读出来，也可能读不出来。比如：某些cache系统，网络游戏其它玩家的数据和你没什么关系，VOIP这样的系统，或是百度搜索引擎（呵呵）。

  2. Eventually 最终一致性：当你写入一个新值后，有可能读不出来，但在某个时间窗口之后保证最终能读出来。比如：DNS，电子邮件、Amazon S3，Google搜索引擎这样的系统。

  3. Strong 强一致性：新的数据一旦写入，在任意副本任意时刻都能读到新值。比如：文件系统，RDBMS，Azure Table都是强一致性的。

  从这三种一致型的模型上来说，我们可以看到，Weak和Eventually一般来说是异步冗余的，而Strong一般来说是同步冗余的，异步的通常意味着更好的性能，但也意味着更复杂的状态控制。同步意味着简单，但也意味着性能下降。 好，让我们由浅入深，一步一步地来看有哪些技术：

  * Master-Slave
    首先是Master-Slave结构，对于这种加构，Slave一般是Master的备份。在这样的系统中，一般是如下设计的：

    1. 读写请求都由Master负责。

    2. 写请求写到Master上后，由Master同步到Slave上。

    从Master同步到Slave上，你可以使用异步，也可以使用同步，可以使用Master来push，也可以使用Slave来pull。 通常来说是Slave来周期性的pull，所以，是最终一致性。这个设计的问题是，如果Master在pull周期内垮掉了，那么会导致这个时间片内的数据丢失。如果你不想让数据丢掉，Slave只能成为Read-Only的方式等Master恢复。

    当然，如果你可以容忍数据丢掉的话，你可以马上让Slave代替Master工作（对于只负责计算的结点来说，没有数据一致性和数据丢失的问题，Master-Slave的方式就可以解决单点问题了） 当然，Master Slave也可以是强一致性的， 比如：当我们写Master的时候，Master负责先写自己，等成功后，再写Slave，两者都成功后返回成功，整个过程是同步的，如果写Slave失败了，那么两种方法，一种是标记Slave不可用报错并继续服务（等Slave恢复后同步Master的数据，可以有多个Slave，这样少一个，还有备份，就像前面说的写三份那样），另一种是回滚自己并返回写失败。（注：一般不先写Slave，因为如果写Master自己失败后，还要回滚Slave，此时如果回滚Slave失败，就得手工订正数据了）你可以看到，如果Master-Slave需要做成强一致性有多复杂。
  * Master-Master
    Master-Master，又叫Multi-master，是指一个系统存在两个或多个Master，每个Master都提供read-write服务。这个模型是Master-Slave的加强版，数据间同步一般是通过Master间的异步完成，所以是最终一致性。 Master-Master的好处是，一台Master挂了，别的Master可以正常做读写服务，他和Master-Slave一样，当数据没有被复制到别的Master上时，数据会丢失。很多数据库都支持Master-Master的Replication的机制。

    另外，如果多个Master对同一个数据进行修改的时候，这个模型的恶梦就出现了——对数据间的冲突合并，这并不是一件容易的事情。看看Dynamo的Vector Clock的设计（记录数据的版本号和修改者）就知道这个事并不那么简单，而且Dynamo对数据冲突这个事是交给用户自己搞的。就像我们的SVN源码冲突一样，对于同一行代码的冲突，只能交给开发者自己来处理。（在本文后后面会讨论一下Dynamo的Vector Clock）
  * 2PC/3PC 两段与三段提交
    这个协议的缩写又叫2PC，中文叫两阶段提交。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。 两阶段提交的算法如下：

    * 两段提交
      * 第一阶段 
        * 询问是否可以执行提交操作。
        * 参与者执行事务的准备工作，如：为资源上锁，预留资源，写undo/redo log...
        * 相应协调者是否同意提交
      * 第二阶段
        * 若是所有的参与者都回答可以提交，协调者发送提交，参与者完成后相应完成或者失败
        * 若是有一个参与者拒绝提交，则协调者发送回滚操作。
    * 三段提交
      他把二段提交的第一个段break成了两段：询问，然后再锁资源。最后真正提交。
      ![](https://coolshell.cn/wp-content/uploads/2014/01/Three-phase_commit_diagram.png)
      三段提交的核心理念是：在询问的时候并不锁定资源，除非所有人都同意了，才开始锁资源。

  **一个网络服务会有三种状态：1）Success，2）Failure，3）Timeout，第三个绝对是恶梦，尤其在你需要维护状态的时候。**

  * Paxos算法和raft算法

# (slides)[http://snarfed.org/transactions_across_datacenters_io.html]

* scaling is hard.
* Context
  * 多宿主： 同时操作多个数据中心
  * 服务可用
  * 数据处理
  * 只读
  * 可读可写(最难的事情)
  * 案例研究：app engine datastore
* 看看有哪些使我要讲的又有哪些使我们顺带说一下的？(说白了就是目录: what's ahead and takeaways)
  * 一致性？
  * 为什么是事务？
  * 为什么要跨数据中心？
  * 多宿主
  * 技术要求和权衡取舍
  * 总结
* 一致性
  * 弱一致性
  * 最终一致性
  * 强一致性
  * 案例：读后写
* 一致性
  * 弱一致性
    * 写后，有可能读不到
    * 尽最大努力保证一致性
    * "Message in a bottle"
    * App engine: memcache
    * VoIP, live on video
    * 实时的多人对战游戏
  * 最终一致性
    * 写后，最终一定会读到的
    * App engine: mail
    * Search engine index 搜索引擎索引
    * DNS, SMTP, snail mail
    * Amazon S3, SimpleDB
  * 强一致性
    * 写后就能读到
    * App Engine: datastore
    * 文件系统
    * RDBMSes
    * Azure tables
* 事务
  